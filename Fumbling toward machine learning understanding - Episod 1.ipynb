{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction #\n",
    "\n",
    "For last few months, I read a lot about machine learning and learnt a bunch of stuff. But the main issue, is that I fell sometimes I missed the basics of deep learning, as more and more libraries abstract many parts of models. So here is the first project. Let's Build a simple neural network with just numpy. Do not use know dataset, just try to modelize a sinus function. I will also have a very naive approach, start with a very simple neural network without bias and then try to correct it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Model description ##\n",
    "\n",
    "Trying to approach the sinus function, I will only use a single layer neural network, with 4 nodes. Why ? well, it is just a guess, let's start simple and if it does not work, I will change the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./img/modele1.png\" width=\"400\" height=\"350\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"./img/modele1.png\",width=400, height=350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First vector X will be the x injected to the sinus function. $\\hat Y$ vector will be our prediction vector. Y vector will be the reference. For instance imagine our first data in X is:$sin(\\frac{\\pi}{2})=1$\n",
    "\n",
    "It will give us $x_1=\\frac{\\pi}{2}$ and $y=1$ , if our model predict 0.9, then $\\hat y_1 = 0.99$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data creation ###\n",
    "Let's introduce first a function that create n random points and their sinus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def randSinVector(n):\n",
    "    np.random.seed(42) # to get same random values eveytimes\n",
    "    x = np.empty([n,2])\n",
    "    theta = 2 * np.pi * np.random.random_sample((n))\n",
    "    x[:,0] = theta\n",
    "    x[:,1] = np.sin(theta)  # value in radian\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=50\n",
    "\n",
    "data = randSinVector(n)\n",
    "X = data[:,0] # will give dimension of (n,), we need (n,1)\n",
    "# see https://stackoverflow.com/questions/36412762/how-to-understand-empty-dimension-in-python-numpy-array\n",
    "X = X[np.newaxis].T # X is a (n,1)  \n",
    "Y = data[:,1]\n",
    "Y = Y[np.newaxis].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Neural Network building ###\n",
    "\n",
    "Let's create the vector representing the weights of the networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layer1_W is the weight vector from input to hidden layer\n",
    "layer1_W = np.random.random((1,4))  # dim = input , number node -> (1 by 4) dimension\n",
    "#layer2_W is the weight vector from layer1 to output layer\n",
    "layer2_W = np.random.random((4,1)) # dim = number node , output -> (4 by 1) dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After each multiplication of matrix by weight, we need to activate the neuron. We will use the sigmoid function. Let's code it and also its derivate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidPrimeLite(s):\n",
    "    return s*(1-s)\n",
    "\n",
    "def sigmoid(s):\n",
    "    # activation function\n",
    "    return 1/(1+np.exp(-s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the mode thanks to the famous backpropagation algorithm (explication below). We will run it with a full batch mode (all the sample at the time), this will provide us n predictions. We will also display the loss during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps:100 - loss:0.6321806432324788\n",
      "steps:200 - loss:0.6321806976986557\n",
      "steps:300 - loss:0.632180697699204\n",
      "steps:400 - loss:0.632180697699204\n",
      "steps:500 - loss:0.632180697699204\n",
      "steps:600 - loss:0.632180697699204\n",
      "steps:700 - loss:0.632180697699204\n",
      "steps:800 - loss:0.632180697699204\n",
      "steps:900 - loss:0.632180697699204\n",
      "steps:1000 - loss:0.632180697699204\n",
      "steps:1100 - loss:0.632180697699204\n",
      "steps:1200 - loss:0.632180697699204\n",
      "steps:1300 - loss:0.632180697699204\n",
      "steps:1400 - loss:0.632180697699204\n",
      "steps:1500 - loss:0.632180697699204\n",
      "steps:1600 - loss:0.632180697699204\n",
      "steps:1700 - loss:0.632180697699204\n",
      "steps:1800 - loss:0.632180697699204\n",
      "steps:1900 - loss:0.632180697699204\n",
      "steps:2000 - loss:0.632180697699204\n",
      "steps:2100 - loss:0.632180697699204\n",
      "steps:2200 - loss:0.632180697699204\n",
      "steps:2300 - loss:0.632180697699204\n",
      "steps:2400 - loss:0.632180697699204\n",
      "steps:2500 - loss:0.632180697699204\n",
      "steps:2600 - loss:0.632180697699204\n",
      "steps:2700 - loss:0.632180697699204\n",
      "steps:2800 - loss:0.632180697699204\n",
      "steps:2900 - loss:0.632180697699204\n",
      "steps:3000 - loss:0.632180697699204\n",
      "steps:3100 - loss:0.632180697699204\n",
      "steps:3200 - loss:0.632180697699204\n",
      "steps:3300 - loss:0.632180697699204\n",
      "steps:3400 - loss:0.632180697699204\n",
      "steps:3500 - loss:0.632180697699204\n",
      "steps:3600 - loss:0.632180697699204\n",
      "steps:3700 - loss:0.632180697699204\n",
      "steps:3800 - loss:0.632180697699204\n",
      "steps:3900 - loss:0.632180697699204\n",
      "steps:4000 - loss:0.632180697699204\n",
      "steps:4100 - loss:0.632180697699204\n",
      "steps:4200 - loss:0.632180697699204\n",
      "steps:4300 - loss:0.632180697699204\n",
      "steps:4400 - loss:0.632180697699204\n",
      "steps:4500 - loss:0.632180697699204\n",
      "steps:4600 - loss:0.632180697699204\n",
      "steps:4700 - loss:0.632180697699204\n",
      "steps:4800 - loss:0.632180697699204\n",
      "steps:4900 - loss:0.632180697699204\n"
     ]
    }
   ],
   "source": [
    "steps = 5000\n",
    "alpha = 0.45\n",
    "for i in range(1,steps):\n",
    "    OUTh = sigmoid(np.dot(X, layer1_W))  # out of layer1 which is a 50x4 dimension\n",
    "\n",
    "    OUTo = sigmoid(np.dot(OUTh, layer2_W))  # prediction of the system (50x1)\n",
    "\n",
    "    # backprog we will use the Sqare error function = Sigma(y_hat-y)^2\n",
    "\n",
    "    # for layer2:\n",
    "    dEtotal_dOUTo = -(Y-OUTo)  # dimension (50,1)\n",
    "    dOUTo_dNETo = sigmoidPrimeLite(OUTo)#OUTo*(1-OUTo)  # dimension (50,1)\n",
    "    #OUTh   dimension (50,4)\n",
    "\n",
    "    dEtotal_dOUTh = np.dot(-(Y-OUTo)*sigmoidPrimeLite(OUTo),layer2_W.T)  # dimension (50,1)\n",
    "    dOUTh_dNETh = sigmoidPrimeLite(OUTh)  # dimension (50,4)\n",
    "    dNETh_dW = X # dimension (50,1)\n",
    "\n",
    "    #layer2_W = layer2_W - alpha * (np.dot(dEtotal_dOUTo.T,dOUTo_dNETo*OUTh)).T ##OLD\n",
    "\n",
    "    layer2_W = layer2_W - alpha * (np.dot((dEtotal_dOUTo*dOUTo_dNETo).T,OUTh)).T\n",
    "\n",
    "    # for layer1:\n",
    "    #layer1_W = layer1_W - alpha * (np.dot((dEtotal_dOUTh * dOUTh_dNETh).T, dNETh_dW)).T  ##OLD\n",
    "    layer1_W = layer1_W - alpha * (X.T.dot(dEtotal_dOUTh*dOUTh_dNETh))\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        OUTh = sigmoid(np.dot(X, layer1_W))  # out of layer1 which is a 50x4 dimension\n",
    "        OUTo = sigmoid(np.dot(OUTh, layer2_W))\n",
    "\n",
    "        print(\"steps:{0} - loss:{1}\".format(i, 0.5*(np.sum(Y-OUTo))*(np.sum(Y-OUTo))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss seems stuck, which is not a good sign. Let's do some tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maths behing backpropagation ###\n",
    "\n",
    "Using backpropagation algorithm without knowing how it works is not a problem in many times. But to perfectly understand what is happening inside a neural network here is math with much explication and calculus. Background needed is matrix multiplication, gradient (partial derivate).\n",
    "\n",
    "#### 1 . Feeforward ####\n",
    "This is a main operation at predicting a value, a class. In our case, based on a certain value $x_n$, our model will generate via a feedword procedure an estimated $\\hat y_n$. How does it works? simply multiply neural network weight by is inputs, and then use an activation function.\n",
    "\n",
    "First layer:\n",
    "\n",
    "Basically, multiply input by matriw weight: $X.W_1$\n",
    "\n",
    "\n",
    "$ X = \\left[ {\\begin{array}{c}\n",
    "   x_1 \\\\ x_2 \\\\ x_3 \\\\ ... \\\\ x_n\\\\\n",
    "  \\end{array} } \\right] $ with dimension (n x 1) and \n",
    "  $ W_1 = \\left[ {\\begin{array}{cccc}\n",
    "   w_{11} & w_{12} & w_{13} & w_{14} \\\\\n",
    "  \\end{array} } \\right] $ with dimension (1 x 4)\n",
    "\n",
    "As right number of matrix X dimension is the same as left number of matrix W dimension ( n x 1 -- 1 x 4) . We can perform a matrix multiplication.\n",
    "\n",
    "$$ X.W_1 = \\left[ {\\begin{array}{cccc}\n",
    "   x_1.w_{11} & x_1.w_{12} & x_1.w_{13} & x_1.w_{14} \\\\ \n",
    "   x_2.w_{11} & x_2.w_{12} & x_2.w_{13} & x_2.w_{14} \\\\ \n",
    "   ... & ... & ... & ... \\\\ \n",
    "   x_n.w_{11} & x_n.w_{12} & x_n.w_{13} & x_n.w_{14} \\\\ \n",
    "  \\end{array} } \\right] = NET_{H1} $$\n",
    "  \n",
    "  We hence get a (n x 4) matrix dimension. Then let's apply the activation function. $ OUT_{h1} = \\sigma(NET_{h1})= \\sigma(X.W_1) $ . This is just taking all the matrix data calculated and pass them through the sigmoid function.\n",
    "  \n",
    "  $$ \\sigma(X.W_1) = OUT_{H1} = \\left[ {\\begin{array}{cccc}\n",
    "   \\sigma(x_1.w_{11}) & \\sigma(x_1.w_{12}) & \\sigma(x_1.w_{13}) & \\sigma(x_1.w_{14}) \\\\ \n",
    "   \\sigma(x_2.w_{11}) & \\sigma(x_2.w_{12}) & \\sigma(x_2.w_{13}) & \\sigma(x_2.w_{14}) \\\\ \n",
    "   ... & ... & ... & ... \\\\ \n",
    "   \\sigma(x_n.w_{11} & \\sigma(x_n.w_{12} & \\sigma(x_n.w_{13} & \\sigma(x_n.w_{14} \\\\ \n",
    "  \\end{array} } \\right] = \\left[ {\\begin{array}{cccc}\n",
    "   OUT_{h11} & OUT_{h12} & OUT_{h13} & OUT_{h14} \\\\ \n",
    "   OUT_{h21} & OUT_{h22} & OUT_{h23} & OUT_{h24} \\\\ \n",
    "   ... & ... & ... & ... \\\\ \n",
    "   OUT_{hn1} & OUT_{hn2} & OUT_{hn3} & OUT_{hn4} \\\\ \n",
    "  \\end{array} } \\right] $$\n",
    "  \n",
    "  \n",
    "  \n",
    "  $OUT_{H1}$ is the output of our hidden layer. Its dimension is (n x 4)\n",
    "\n",
    "  Let's now calculate the final output of our network. Principle is the same, multiply the weights by the input: $OUT_{H1}.W_{2}$ . Dimension of $W_2$ is (4 x 1).\n",
    "  \n",
    "  $$ OUT_{H1}.W_2 = \\left[ {\\begin{array}{cccc}\n",
    "   OUT_{h11} & OUT_{h12} & OUT_{h13} & OUT_{h14} \\\\ \n",
    "   OUT_{h21} & OUT_{h22} & OUT_{h23} & OUT_{h24} \\\\ \n",
    "   ... & ... & ... & ... \\\\ \n",
    "   OUT_{hn1} & OUT_{hn2} & OUT_{hn3} & OUT_{hn4} \\\\ \n",
    "  \\end{array} } \\right] . \\left[ {\\begin{array}{cccc}\n",
    "   w_{21} \\\\ w_{22} \\\\ w_{23} \\\\ w_{24} \\\\\n",
    "  \\end{array} } \\right] = \\left[ {\\begin{array}{cccc}\n",
    "   OUT_{h11}.w_{21}+OUT_{h12}.w_{22}+OUT_{h13}.w_{23}+OUT_{h14}.w_{24} \\\\\n",
    "   OUT_{h21}.w_{21}+OUT_{h22}.w_{22}+OUT_{h23}.w_{23}+OUT_{h24}.w_{24} \\\\\n",
    "   ...... \\\\\n",
    "   OUT_{hn1}.w_{21}+OUT_{hn2}.w_{22}+OUT_{hn3}.w_{23}+OUT_{hn4}.w_{24} \\\\\n",
    "  \\end{array} } \\right] $$\n",
    "  \n",
    "  \n",
    "  $$ \\sigma(OUT_{H1}.W_2) = OUT_{O} = \n",
    "  \\left[ {\\begin{array}{cccc}\n",
    "   \\sigma(OUT_{h11}.w_{21}+OUT_{h12}.w_{22}+OUT_{h13}.w_{23}+OUT_{h14}.w_{24}) \\\\\n",
    "   \\sigma(OUT_{h21}.w_{21}+OUT_{h22}.w_{22}+OUT_{h23}.w_{23}+OUT_{h24}.w_{24}) \\\\\n",
    "   ..... \\\\\n",
    "   \\sigma(OUT_{h41}.w_{21}+OUT_{h42}.w_{22}+OUT_{h43}.w_{23}+OUT_{h44}.w_{24}) \\\\\n",
    "  \\end{array} } \\right] = \\left[ {\\begin{array}{cccc}\n",
    "   \\hat{y_1}\\\\\n",
    "   \\hat{y_2}\\\\ \\\\\n",
    "   ..... \\\\\n",
    "   \\hat{y_n}\\\\ \\\\\n",
    "  \\end{array} } \\right] $$\n",
    "  \n",
    "  And this is it! For instance, for $x_2$ we get the prediction $\\hat{y_2}$. As a code it looks like only 2 lines:\n",
    "  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTh = sigmoid(np.dot(X, layer1_W))  # out of layer1 which is a 50x4 dimension\n",
    "\n",
    "OUTo = sigmoid(np.dot(OUTh, layer2_W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 . Backpropagation #### \n",
    " This is the trickiest part. Many times, one can just copy-paste a part of algorithm found on internet, but I really wanted to understand the algorithm as if I need fine tuning, understand what would be impacted.\n",
    " \n",
    " What is the optimization problem? \n",
    " \n",
    " We want a prediction to be as closed as possible Ã  reality : $ \\hat{y} \\simeq y $ which can be translated as $ y - \\hat{y} \\simeq 0 $\n",
    " \n",
    " The optimization must be done for every input X :\n",
    " \n",
    " $ (y_1 - \\hat{y_1}) +(y_2 - \\hat{y_2}) +(y_3 - \\hat{y_3}) + ....+(y_n - \\hat{y_n}) = \\sum_{i=1}^{n}(y_i-\\hat{y_i}) $\n",
    "  \n",
    "  \n",
    "  The difference $y_i-\\hat{y_i}$ may be positive or negative. To normalize, we square it. And as we will derivate this equation, we place a 1/2, just for convenience for next calculus. Finally, we get:\n",
    "  \n",
    " $E = \\frac{1}{2}.\\sum_{i=1}^{n}(Y_i-\\hat{Y_i})^2 $  which is called the mean square error.\n",
    " \n",
    " \n",
    " As a reminder, we want E to be as small as possible. Our input X and output Y cannot be optimized, the only modification that can be done on our model is on the Weigth : $W_1$ ; $W_2$\n",
    " \n",
    " Let's start with the last layer $W_2$, how does a modification of $W_2$ impact E?\n",
    " \n",
    " if $\\frac{\\partial{E}}{\\partial{W_2}}$ is negative, then E curve is decreasing, otherwise E is increasing when it is positive. If E is decreasing, then we should increase $W_2$, and reversely if E is increasing we should decrease $W_2$.\n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial{E}}{\\partial{W_2}} = \\frac{\\frac{1}{2}.\\partial{\\sum_{i=1}^{n}(Y_i-\\hat{Y_i})^2}}{\\partial{W_2}}$ would be what we need to compute, but this is not a easy task. We will hence use the chaine rule like this:\n",
    "\n",
    "$\\frac{\\partial{E}}{\\partial{W_2}} = \\frac{\\partial{E}}{\\partial{OUT_{o}}}. \\frac{\\partial{OUT_{o}}}{\\partial{NET_{h1}}} .\\frac{\\partial{NET_{h1}}}{\\partial{W_2}}$ Here, we have 3 terms that are easier to calculate:\n",
    "\n",
    "\n",
    "$\\frac{\\partial{E}}{\\partial{OUT_{o}}}= \\frac{\\frac{1}{2}.\\partial{(Y_i-OUT_o)^2}}{\\partial{OUT_{o}}} = \\frac{1}{2}.2.(Y-OUT_o).(-1) = - (Y-OUT_o) $\n",
    "\n",
    "$\\frac{\\partial{OUT_{o}}}{\\partial{NET_{h1}}}= \\frac{\\partial{\\sigma(OUT_{H1}.W_2)}}{\\partial{NET_{h1}}}= \\frac{\\partial{\\frac{1}{1+\\exp{-NET_O1}}}}{\\partial{NET_{h1}}} = OUT_o.(1-OUT_o) $\n",
    "\n",
    "\n",
    "$\\frac{\\partial{NET_{h1}}}{\\partial{W_2}}= \\frac{\\partial{OUTH1.W2}}{\\partial{W_2}} = OUT_{H1} $\n",
    "\n",
    "Which will give us:\n",
    "\n",
    "$\\frac{\\partial{E}}{\\partial{W_2}} = - (Y-OUT_o).OUT_o.(1-OUT_o).OUT_{H1}$ <= Exp1\n",
    "\n",
    "We will then update W2 according the following calculated above. To make sure that the step is not too big we will add a delta. So if $\\frac{\\partial{E}}{\\partial{W_2}}$ is negative, it means that the slope of E is going down when we increase W2, so the expression is :\n",
    "\n",
    "$W2^+ = W2 - \\alpha . \\frac{\\partial{E}}{\\partial{W_2}}$  As there is a minus in front of EXp1, we place a new minus in frint of alpha as we want W2 to increase.\n",
    "\n",
    "We just updated our first layer of weight. How to update the first layer? Same principle. How does W1 affect E?\n",
    "\n",
    "\n",
    "$\\frac{\\partial{E}}{\\partial{W_1}} = \\frac{\\partial{E}}{\\partial{OUT_{H1}}}. \\frac{\\partial{OUT_{H1}}}{\\partial{NET_{h1}}} .\\frac{\\partial{NET_{h1}}}{\\partial{W_1}}$  WE need to use chaine rule again to calculate the first term:\n",
    "\n",
    "$ \\frac{\\partial{E}}{\\partial{OUT_{H1}}} = \\frac{\\partial{E}}{\\partial{NET_{O1}}}.\\frac{\\partial{NET_{O1}}}{\\partial{OUT_{H1}}}$\n",
    "\n",
    "$\\frac{\\partial{E}}{\\partial{NET_{01}}} = \\frac{\\partial{E}}{\\partial{OUT_{O1}}} . \\frac{\\partial{OUT_{O1}}}{\\partial{NET_{01}}} = - (Y-OUT_o) . OUT_o.(1-OUT_o)$\n",
    "\n",
    "$\\frac{\\partial{NET_{O1}}}{\\partial{OUT_{H1}}} = \\frac{\\partial{(W_5.OUT_{H1}+...)}}{\\partial{OUT_{H1}}}=W5 $\n",
    "\n",
    "$\\frac{\\partial{OUT_{H1}}}{\\partial{NET_{h1}}} = \\frac{\\partial{\\sigma(X.W_1)}}{\\partial{NET_{h1}}}= \\frac{\\partial{\\frac{1}{1+\\exp{-NET_{h1}}}}}{\\partial{NET_{h1}}} = NET_{h1}.(1-NET_{h1}) $\n",
    "\n",
    "$ \\frac{\\partial{NET_{h1}}}{\\partial{W_1}} = \\frac{\\partial{(W_1.NET_{h1}+...)}}{\\partial{W_1}}=X $\n",
    "\n",
    "Finally:\n",
    "\n",
    "$\\frac{\\partial{E}}{\\partial{W_1}} = - (Y-OUT_o) . OUT_o.(1-OUT_o) . W5 . NET_{h1}.(1-NET_{h1}) . X$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Test the model ###\n",
    "\n",
    "Let's generate a size 3 vector with some values and compare it to the model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.17202415]\n",
      " [0.21387665]\n",
      " [0.20296691]]\n",
      "result should be[[-0.43030122 -0.14773276 -0.9921147 ]]\n"
     ]
    }
   ],
   "source": [
    "X_test = np.array([(np.pi/2),(np.pi/6),0.77])\n",
    "\n",
    "X_test = X_test[np.newaxis]\n",
    "output1 = sigmoid(np.dot(X_test.T,layer1_W))  \n",
    "\n",
    "test_prediction = sigmoid(np.dot(output1,layer2_W))\n",
    "\n",
    "print(test_prediction)\n",
    "\n",
    "print(\"result should be\"+str(np.sin(2 * np.pi*X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOUM!#### \n",
    "Not good at all. Let's try improve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Investigate the problem ###\n",
    "\n",
    "Let's try to display data and prediction on a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112351198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig =plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(X,Y,color='lightblue',marker='o')\n",
    "ax.scatter(X,  OUTo,\n",
    "               color='darkgreen',\n",
    "               marker='^')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hum, confirmed this is horrible..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus on the network. What we want is basically a prediction of sinus function as output. Sinus value are between -1 and 1. The first obvious mistake is that with our ouput layer we are applying an activation function Sigmoid. Sigmoid domain is from 0 to 1, meaning that negative value will never show up ! What we can try is to use the tanh activation function which has domain from -1 to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def dtanh(y):\n",
    "    return 1 - y*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps:100 - loss:772.7072285282873\n",
      "steps:200 - loss:879.2478962050275\n",
      "steps:300 - loss:488.34552890963573\n",
      "steps:400 - loss:927.0465810691587\n",
      "steps:500 - loss:924.9772651326291\n",
      "steps:600 - loss:881.4360058370088\n",
      "steps:700 - loss:929.2303908179531\n",
      "steps:800 - loss:921.7693639879035\n",
      "steps:900 - loss:1297.3600725540166\n",
      "steps:1000 - loss:27.447467757745358\n",
      "steps:1100 - loss:915.6325378989238\n",
      "steps:1200 - loss:909.6386632695841\n",
      "steps:1300 - loss:912.140571788562\n",
      "steps:1400 - loss:875.2358776390239\n",
      "steps:1500 - loss:820.2439595776176\n",
      "steps:1600 - loss:924.7495429462308\n",
      "steps:1700 - loss:1381.5394617468432\n",
      "steps:1800 - loss:735.6299706010606\n",
      "steps:1900 - loss:917.7771425360946\n",
      "steps:2000 - loss:932.9072890339606\n",
      "steps:2100 - loss:809.4926544666039\n",
      "steps:2200 - loss:924.4487160882597\n",
      "steps:2300 - loss:931.0540556837909\n",
      "steps:2400 - loss:897.4537096189678\n",
      "steps:2500 - loss:926.180352602585\n",
      "steps:2600 - loss:1040.9921527541655\n",
      "steps:2700 - loss:911.0097644817422\n",
      "steps:2800 - loss:917.8011516678781\n",
      "steps:2900 - loss:911.9342021943955\n",
      "steps:3000 - loss:908.177304681629\n",
      "steps:3100 - loss:644.2852434455673\n",
      "steps:3200 - loss:112.10838088237698\n",
      "steps:3300 - loss:964.4028860923617\n",
      "steps:3400 - loss:845.9931856083728\n",
      "steps:3500 - loss:922.6079224979486\n",
      "steps:3600 - loss:1321.3718899211449\n",
      "steps:3700 - loss:952.1504783386401\n",
      "steps:3800 - loss:665.4247778854792\n",
      "steps:3900 - loss:166.6445462929495\n",
      "steps:4000 - loss:930.007104934383\n",
      "steps:4100 - loss:927.0170867255231\n",
      "steps:4200 - loss:827.8038988717748\n",
      "steps:4300 - loss:1444.9501896916113\n",
      "steps:4400 - loss:737.0747894627806\n",
      "steps:4500 - loss:757.1363109840349\n",
      "steps:4600 - loss:924.7460642067773\n",
      "steps:4700 - loss:895.1632092412141\n",
      "steps:4800 - loss:927.3528430348374\n",
      "steps:4900 - loss:821.4974432815526\n"
     ]
    }
   ],
   "source": [
    "steps = 5000\n",
    "alpha = 0.05\n",
    "for i in range(1,steps):\n",
    "    OUTh = tanh(np.dot(X, layer1_W))  # out of layer1 which is a 50x4 dimension\n",
    "\n",
    "    OUTo = tanh(np.dot(OUTh, layer2_W))  # prediction of the system (50x1)\n",
    "\n",
    "    # backprog we will use the Sqare error function = Sigma(y_hat-y)^2\n",
    "\n",
    "    # for layer2:\n",
    "    dEtotal_dOUTo = -(Y-OUTo)  # dimension (50,1)\n",
    "    dOUTo_dNETo = dtanh(OUTo)#OUTo*(1-OUTo)  # dimension (50,1)\n",
    "    #OUTh   dimension (50,4)\n",
    "\n",
    "    dEtotal_dOUTh = np.dot(-(Y-OUTo)*dtanh(OUTo),layer2_W.T)  # dimension (50,1)\n",
    "    dOUTh_dNETh = dtanh(OUTh)  # dimension (50,4)\n",
    "    dNETh_dW = X # dimension (50,1)\n",
    "\n",
    "    #layer2_W = layer2_W - alpha * (np.dot(dEtotal_dOUTo.T,dOUTo_dNETo*OUTh)).T ##OLD\n",
    "\n",
    "    layer2_W = layer2_W - alpha * (np.dot((dEtotal_dOUTo*dOUTo_dNETo).T,OUTh)).T\n",
    "\n",
    "    # for layer1:\n",
    "    #layer1_W = layer1_W - alpha * (np.dot((dEtotal_dOUTh * dOUTh_dNETh).T, dNETh_dW)).T  ##OLD\n",
    "    layer1_W = layer1_W - alpha * (X.T.dot(dEtotal_dOUTh*dOUTh_dNETh))\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        OUTh = tanh(np.dot(X, layer1_W))  # out of layer1 which is a 50x4 dimension\n",
    "        OUTo = tanh(np.dot(OUTh, layer2_W))\n",
    "\n",
    "        print(\"steps:{0} - loss:{1}\".format(i, 0.5*(np.sum(Y-OUTo))*(np.sum(Y-OUTo))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pffff . No improve, seems that the model is not complex enough. Add some more neurons in the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layer1_W is the weight vector from input to hidden layer\n",
    "layer1_W = np.random.random((1,8))  # dim = input , number node -> (1 by 8) dimension\n",
    "#layer2_W is the weight vector from layer1 to output layer\n",
    "layer2_W = np.random.random((8,1)) # dim = number node , output -> (4 by 8) dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps:100 - loss:932.1828637091047\n",
      "steps:200 - loss:912.2550382409053\n",
      "steps:300 - loss:899.6415314294369\n",
      "steps:400 - loss:768.6053479514875\n",
      "steps:500 - loss:844.5366232246242\n",
      "steps:600 - loss:1583.17476907526\n",
      "steps:700 - loss:935.9494413618061\n",
      "steps:800 - loss:945.0812675421755\n",
      "steps:900 - loss:921.0378904719922\n",
      "steps:1000 - loss:908.9924045635471\n",
      "steps:1100 - loss:935.3042400048673\n",
      "steps:1200 - loss:1489.056762045091\n",
      "steps:1300 - loss:902.5133970036619\n",
      "steps:1400 - loss:903.7115439962251\n",
      "steps:1500 - loss:454.43237201091375\n",
      "steps:1600 - loss:922.9179356227324\n",
      "steps:1700 - loss:942.9717857762267\n",
      "steps:1800 - loss:878.0895917475058\n",
      "steps:1900 - loss:1453.8436125059645\n",
      "steps:2000 - loss:919.9557823838653\n",
      "steps:2100 - loss:872.5196517080215\n",
      "steps:2200 - loss:1397.518614266492\n",
      "steps:2300 - loss:845.621690323645\n",
      "steps:2400 - loss:782.5011920378134\n",
      "steps:2500 - loss:896.5041469566077\n",
      "steps:2600 - loss:839.7184326456381\n",
      "steps:2700 - loss:924.488108622547\n",
      "steps:2800 - loss:1589.8413024099211\n",
      "steps:2900 - loss:947.1288929967257\n",
      "steps:3000 - loss:944.5894626879482\n",
      "steps:3100 - loss:1544.46737472629\n",
      "steps:3200 - loss:946.9817830607584\n",
      "steps:3300 - loss:944.6419791237679\n",
      "steps:3400 - loss:978.561605191675\n",
      "steps:3500 - loss:930.8179399837596\n",
      "steps:3600 - loss:939.0535521743843\n",
      "steps:3700 - loss:1576.2174242509786\n",
      "steps:3800 - loss:920.3521425635383\n",
      "steps:3900 - loss:922.1825937700703\n",
      "steps:4000 - loss:946.4833397610741\n",
      "steps:4100 - loss:939.4137001174703\n",
      "steps:4200 - loss:936.7653898132131\n",
      "steps:4300 - loss:946.2806384153137\n",
      "steps:4400 - loss:940.9037896378046\n",
      "steps:4500 - loss:934.280815862314\n",
      "steps:4600 - loss:905.0975146412303\n",
      "steps:4700 - loss:941.2532594809481\n",
      "steps:4800 - loss:812.9760589488333\n",
      "steps:4900 - loss:946.0179834684949\n"
     ]
    }
   ],
   "source": [
    "steps = 5000\n",
    "alpha = 0.05\n",
    "\n",
    "for i in range(1,steps):\n",
    "    #for (Xb, Yb) in next_batch(X, Y):\n",
    "        OUTh = tanh(np.dot(X, layer1_W))  # out of layer1 which is a 50x4 dimension\n",
    "\n",
    "        OUTo = tanh(np.dot(OUTh, layer2_W))  # prediction of the system (50x1)\n",
    "\n",
    "        # backprog we will use the Sqare error function = Sigma(y_hat-y)^2\n",
    "\n",
    "        # for layer2:\n",
    "        dEtotal_dOUTo = -(Y-OUTo)  # dimension (50,1)\n",
    "        dOUTo_dNETo = dtanh(OUTo)#OUTo*(1-OUTo)  # dimension (50,1)\n",
    "        #OUTh   dimension (50,4)\n",
    "\n",
    "        dEtotal_dOUTh = np.dot(-(Y-OUTo)*dtanh(OUTo),layer2_W.T)  # dimension (50,1)\n",
    "        dOUTh_dNETh = dtanh(OUTh)  # dimension (50,4)\n",
    "        dNETh_dW = X # dimension (50,1)\n",
    "\n",
    "        #layer2_W = layer2_W - alpha * (np.dot(dEtotal_dOUTo.T,dOUTo_dNETo*OUTh)).T ##OLD\n",
    "\n",
    "        layer2_W = layer2_W - alpha * (np.dot((dEtotal_dOUTo*dOUTo_dNETo).T,OUTh)).T\n",
    "\n",
    "        # for layer1:\n",
    "        #layer1_W = layer1_W - alpha * (np.dot((dEtotal_dOUTh * dOUTh_dNETh).T, dNETh_dW)).T  ##OLD\n",
    "        layer1_W = layer1_W - alpha * (X.T.dot(dEtotal_dOUTh*dOUTh_dNETh))\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            OUTh = tanh(np.dot(X, layer1_W))  # out of layer1 which is a 50x4 dimension\n",
    "            OUTo = tanh(np.dot(OUTh, layer2_W))\n",
    "\n",
    "            print(\"steps:{0} - loss:{1}\".format(i, 0.5*(np.sum(Y-OUTo))*(np.sum(Y-OUTo))))\n",
    "        OUTh = tanh(np.dot(X, layer1_W))  # out of layer1 which is a 50x4 dimension\n",
    "        OUTo = tanh(np.dot(OUTh, layer2_W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAH81JREFUeJzt3X+cXXV95/HXOxlCEgsTMDGkkJBIeVClWuLOsuzqQ1sEDa0PgqwiCDa22uhD6Xbrw8oPu9oFfRRtu7hrqSWGYBQWUMQlpSCVH66/AJmYBAg0JQQYkk7C8GvETcwwM5/9454b7725v+aeM/fn+/l4zGPuOed77v3ce2bu53y/33O+X0UEZmZmeTNaHYCZmbUXJwYzMyvixGBmZkWcGMzMrIgTg5mZFXFiMDOzIk4MZmZWxInBzMyKODGYmVmRvlYH0Ij58+fH0qVLWx2GmVlH2bhx43MRsaBWuY5MDEuXLmVwcLDVYZiZdRRJT9dTzk1JZmZWxInBzMyKODGYmVkRJwYzMyvixGBmZkWcGMzMrIgTg5mZFXFiMDOzIpkkBknrJD0r6ZEK2yXpf0naLukhSW8q2LZK0uPJz6os4pkOwy8Nc9ylx7HlmS0cd+lx7B7dXVf5SuXKba+0z+ahzRzykUN46JmHypbbPLSZvtV9LP7U4gPrSsvUiqeSodG9XL/lIY765FKu3/IwQ6N7K5YtjbNwudHXL3TgGAzVdwzqfb7do7undDzqeb6ycSd/O5XiT/MZFX42x150LEsvWsqWZ7aw9KKlHHvxsQ1/VtVianRbPdvTxJU2trRxNrJP6f9O/nnSHr9GZVVj+Bqwosr2M4Djk5/VwFcAJB0JfBb4D8DJwGclHZFRTKmUHtzLb7ucp557ivO/ej5PPfcUl992edX98+UrlSu3vdI+F6y9gPHJcd6/9v1ly12w9gImYoKdL+48sK60TK14yhka3cumPaN844d/y7OjQ3zjh3/Lpj2jFZNDaZyFy428fqkDx+Ca+o5Bvc93+W2XT+l41PN8ZeNO/nYqxZ/mMyr8bIZeGOLpF57m/K+ez9MvPM3Q80MNf1bVYmp0Wz3b08SVNra0cTayT+n/Tv550h6/RikisnkiaSlwW0T8VpltVwPfj4gbkuVtwO/kfyLiI+XKVTIwMBBZD4kx/NIwb/niW/jxRT/mqP6j+Nh1H+PqH1zNR9/2Uf7i9/+C1176Wn75yi8PlJ9zyBx2/NUOjuo/quxz5cuXK1due0SU3Wfz0GaWX778wL53/dldvOvv3nWg3IYLN3D6lacf2H5o36E8cMkDnHLFKQfK3HfxfUXLleIudccTe/i3l4b52FdPYWxiP7P6ZvP3f3w/v95/FGcct7CobGmcN/3xTbzvq+8rimv/+P4pvX6lzzSv0ecqfb7ZfbMB+OV47eNRz/MVli8Xd7n4a/3N1PteKpndN5snr3hySp9VtZga3VbP9jRxpY1tKq+T1T6l/ztbPrOFBYctYNkly9g/vh9o7PiVI2ljRAzUKtesPoajgWcKlncm6yqtP4ik1ZIGJQ2OjIxkHmBhlh9+aZhrf3ItkzHJtT++lktuuYTJycmi8hOTE1XPVvLly5Urt73SPhesvaBo3/de/d6icudcfU7R9v3j+zn/mvOLy6w5j/Fk+ZXJCT51y2fq+kz2jU/yrfuuZJLcycNkTPKtn1zJvvHJg8qes+a8ouUPrPvAQXGVvrepKPx88hp9rtLnGxsfY2xirOg5ax3Das9XWL5c3OXin+rrVXrtSsbGx6b8WVWLqdFt9WxPE1fa2NLG2cg+pf/j+Rr2KxOvHFjXyPFLo1k1htuAKyLiR8ny3cBF5GoMsyPic8n6/wbsi4i/qfZaWdcYSrP8OQPncMNPb2BsYoxZM2cxMTnBREwctF+ts5Vy5cptn33IbCLiwJdnfp/S2kBWZvXN5seXbmVg8Wurlrt+y0P80VdOZmziV3HN6pvNuo/+lPN/+w0H1t3x6H383pX/qe7Xz/KMOO0ZZzmVjkc9NcTC8oU1tUrmHDKH+y65j1P+6pSGakP11BYOvK8pnHVW+zsurE1NZVulv/+pHMNG/r/qjW0qr9NIbOWU1hbyZs2cdeBkJS+LWkO71Rh2AYsLlo9J1lVa31SlWf66+687cFDGJsbKJoV82WpnK+XKlds+Nj7GK+OvHLRPaW0gK5MxyV/+Y+2zj+9u/PKB2kLhvnf+7MtF6z7y9Q9O6fWzPCNOe8ZZTqXjUU8NsbB8Yc2tkonJCc7/6sHlGjlDrWUqZ53V/o4b3VbredPGlSbuqb5OI7GVU1pbyCtNCtDcWkOzEsMG4A+Sq5NOAUYjYhi4E3iHpCOSTud3JOuaJt9sVE8iKDU2Mcatm28tWrdhy4aDDmphuXLbJ2OSSSYP2ufFvS9O6b3Ua3xijPse/27Ncvc+dgfjJbGOT4xxz6O3F63b+cL2Kb1+uc+tmnKfWaPPVev5oPLxqPQ6lY75Y8OPVX2dA+V2H1yu3vdV670UmmSy7s+q2t9xo9tqPW/auNLEPdXXaSS2ch7b/VjFbaWmcvzSyqQpSdIN5JqF5gN7yF1pdAhARPyDJAF/R+7Kpb3AH0bEYLLvHwGXJk/1+Yi4ttbrZdmU9LHrPsY1P7qm5j/X0fOOZudf78zkNafTHU/sKdsHUGpO34yDOpAbdcu24brLzumbwYnzD2NJ/9xMXtvM6ldvU1ImE/VExHk1tgfw8Qrb1gHrsoijEZXOuDolEUDustKtz71cV0IAmCk4cf5hmb3+rBlibLK+E4x945Ns2jMK4ORg1qY6cga3LHXKl38lP3j6OZ775Ss1ywkIpueM/Y2vOZyf7R6lvrQEEwFbn3vZicGsTfV8Yuhkm4ZfqispzBQsX9g/bV/E+eedSq2l3nJm1nw9O1ZSFkM0tNpTP99Xs8ycvhnTmhTylvTP5YzjFnL2CYtYdvicuuIys/bUszWGwhvarjr/qlaHU7d6+xOy7FyequWL5gG5xFWu5yHrPg4zy1ZPnraV3tncKbWG/NhF9TTDtPqLd/miebz7hEWcfcIiBo7qP1BDaFYNxswa15M1hnK3rXdCrWHrcy8zUcfFP/NnH9JWX7xL+ue2VTxmVl3P1RjK3dDWKbWGWjUFAcsOn8Nbj53fnIDMrCv1XGLIejC2ZqrUYTunbwZnn7CId5+w6ED7vplZo3ouMaS9Jb/ZNg2/xHe2DXPLtmH2jU+iku3uyDWzrPVcH0Mn3dC2afglniy5JDWAmRITER5ewsymRc8lhk5S6T6FyQjOPmFRk6Mxs17Rc01JnaTSBUjZzKBhZlaeE0MbK+1PqLXezCwLTgxtbGmFoSUqrTczy4L7GNpE4VAX+U7l0qElRC4p+JJUM5tOmSQGSSuA/wnMBNZGxBUl268EfjdZnAu8JiLmJdsmgIeTbUMRcWYWMXWS/FAX+buaC+csWL5onhOBmTVV6sQgaSZwFXA6sBN4UNKGiHg0XyYi/qyg/J8AhbNf74uIk9LG0cnKDXXhOQuKlatR+bMxmx5Z9DGcDGyPiB0RMQbcCKysUv484IYMXrdrVBrqwnMW5JQOHpivUQ2N7m1xZGbdKYvEcDTwTMHyzmTdQSQdCywD7ilYPVvSoKT7JZ1V6UUkrU7KDY6MjGQQdvuoNtSFVa9RmVn2mv3Ncy5wc0RMFKw7Npmc+v3AlyQdV27HiFgTEQMRMbBgwYJmxNo0J84/jJkl16B6qItfqVajcq3BLHtZJIZdwOKC5WOSdeWcS0kzUkTsSn7vAL5Pcf9DT1jSP5flCz1nQSXVak5uUjLLXhZXJT0IHC9pGbmEcC65s/8ikn4TOAK4r2DdEcDeiNgvaT7wZuCLGcTU1ip1pDoRlHfi/MOKrtoq5E56s+ylTgwRMS7pQuBOcperrouIrZIuAwYjYkNS9Fzgxogo/Pd+HXC1pElytZcrCq9m6kbVLk31l1t5+c9lcPdo2e3upDfLVib3MUTE7cDtJes+U7L8l2X2+wnwhixi6BS+NLUxS/rnVpzr2p30Ztnyf1ST+dLUxrmT3qw5PCRGk83pm+Gz3gbla1S+0c1sejkxNFm5jlSf9dbPnfRm08+Jocl81mtm7c6JoQV81mtm7cwN22ZmVsSJwczMijgxmJlZEfcxTAPPHWBmncyJIWMe8sLMOp2bkjLmuQPMrNM5MWTMQ16YWadzYsiYZ2Mzs07nb6uMeaA3M+t07nzOmIe8MLNOl0mNQdIKSdskbZd0cZntH5Q0Imlz8vPhgm2rJD2e/KzKIp5WW9I/lzOOW8jZJyzijOMWOimYWUdJXWOQNBO4Cjgd2Ak8KGlDmZnYboqIC0v2PRL4LDAABLAx2ffFtHGZmVljsqgxnAxsj4gdETEG3AisrHPfdwLfi4gXkmTwPWBFBjGZmVmDskgMRwPPFCzvTNaV+s+SHpJ0s6TFU9zXzMyapFlXJf0jsDQi3kiuVrB+qk8gabWkQUmDIyMjmQdoZmY5WSSGXcDiguVjknUHRMTzEbE/WVwL/Lt69y14jjURMRARAwsWLMggbDMzKyeLxPAgcLykZZJmAecCGwoLSFpUsHgm8Fjy+E7gHZKOkHQE8I5knZmZtUjqq5IiYlzSheS+0GcC6yJiq6TLgMGI2AD8F0lnAuPAC8AHk31fkHQ5ueQCcFlEvJA2JjMza5wionapNjMwMBCDg4OtDsPMrKNI2hgRA7XK+c5n60meM8OsMicG6zmeM8OsOieGKfBZZneoNmeGj6eZE0PdfJbZPTxnhll1Hna7Tp6ZrXt4zgyz6vyfUCefZXaPcnNmQO5Y3vHEHoZG9zY/KLM24qakOgyN7kXkhn8t5bPMzlM6Z0YhNxGaucZQU75voVxS8MxsnSs/Z0a5xO4mQut1Tgw1lOtbABCwfGG/zyo7nJsIzQ7mxFBDpS+IwE0N3cAd0WYH819/Df7i6G7lOqLdRGi9zt9uNfiLo7st6Z/L8oX9BxL9nL4ZbiK0nuerksoovcN5yWFz2L13v+947lJL+uf6eJoVcGIoUe4O56GX9/ks0sx6hhNDCY+jY+Bxsay3ZdLHIGmFpG2Stku6uMz2T0h6VNJDku6WdGzBtglJm5OfDaX7NpsvX7R8rTF/zPM3vfmOaOsVqRODpJnAVcAZwOuB8yS9vqTYJmAgIt4I3Ax8sWDbvog4Kfk5M208afkqJPO4WNbrsvi2OxnYHhE7ImIMuBFYWVggIu6NiPzp1v3AMRm87rTwVUjmWqP1uiwSw9HAMwXLO5N1lXwIuKNgebakQUn3Szorg3hS8eWL5lqj9bqmdj5LugAYAN5WsPrYiNgl6bXAPZIejognyuy7GlgNsGTJkkzjKtfReMZxCzN9DescJ84/rOjKNHCt0XpLFqdAu4DFBcvHJOuKSDoN+DRwZkTsz6+PiF3J7x3A94Hl5V4kItZExEBEDCxYsCCDsHPc0WilXGu0XpdFjeFB4HhJy8glhHOB9xcWkLQcuBpYERHPFqw/AtgbEfslzQfeTHHH9LTz5alWjm96s16WOjFExLikC4E7gZnAuojYKukyYDAiNgB/Dfwa8C1JAEPJFUivA66WNEmu9nJFRDyaNqapcEejmVmxTPoYIuJ24PaSdZ8peHxahf1+ArwhixgaNadvRtkk4I5GM+tVPf/t58tTzcyK9fyQGKXTPHr4AzPrdT2fGMAdjWZmhXq+KcnMzIr1ZI3BI2eamVXWc4mh3HwLm/aMAp7D2cwMerApySNnmplV13OJwTe0mZlV13OJwSNnmplV13Pfhr6hzcysup7rfPYNbWZm1fVcYgDf0GZmVk3PNSWZmVl1PVljMMuab5q0buLEYJaSb5q0buOmJLOUfNOkdZtMEoOkFZK2Sdou6eIy2w+VdFOy/QFJSwu2XZKs3ybpnVnEY9ZMvmnSuk3qxCBpJnAVcAbweuA8Sa8vKfYh4MWI+A3gSuALyb6vJzdH9InACuDvk+cz6xi+adK6TRZ/uScD2yNiR0SMATcCK0vKrATWJ49vBt6u3OTPK4EbI2J/RDwJbE+ez6xj+KZJ6zZZJIajgWcKlncm68qWiYhxYBR4dZ37AiBptaRBSYMjIyMZhG2WjSX9c1m+sP9ADWFO3wyWL+x3x7N1rI65Kiki1gBrAAYGBqJGcbOm8k2T1k2yqDHsAhYXLB+TrCtbRlIf0A88X+e+ZmbWRFkkhgeB4yUtkzSLXGfyhpIyG4BVyeP3APdERCTrz02uWloGHA/8NIOYzMysQambkiJiXNKFwJ3ATGBdRGyVdBkwGBEbgGuAb0jaDrxALnmQlPsm8CgwDnw8IibSxmRmZo1T7sS9swwMDMTg4GCrwzAz6yiSNkbEQK1yHdP5bNaJPIaSdSInBrNp4jGUrFP51kyzaeIxlKxTOTGYTZNqYygNje5tcjRm9XNiMJsm1cZK2rRn1MnB2pYTg9k0KTeGUp6blKydOTGYTZP8GEqVuEnJ2pUTg9k0WtI/101K1nGcGMymmZuUrNP4PgazaZa/Z2Fw92jZ7Z7pzcpp5c2RrjGYNUG1JiXP9Gal8jdH5k8a8jdHNqvZ0X+RZk3imd6sXq2+OdJNSWZNkm8G8NhJVku1myObwYnBrIk805vVUq25qFnNjm5KMjNrI9Wai5rV7JgqMUg6UtL3JD2e/D6iTJmTJN0naaukhyS9r2Db1yQ9KWlz8nNSmnjMzDpdteaiTrkq6WLg7og4Hrg7WS61F/iDiDgRWAF8SdK8gu1/HhEnJT+bU8ZjZtbR2uHqtbSvtBJYnzxeD5xVWiAi/jUiHk8e/xvwLLAg5euamXWldrh6LW1iWBgRw8nj3cDCaoUlnQzMAp4oWP35pInpSkmHVtl3taRBSYMjIyMpwzYza0/5MbbyNYQ5fTNYvrC/qRct1JzzWdJdwFFlNn0aWB8R8wrKvhgRB/UzJNsWAd8HVkXE/QXrdpNLFmuAJyLislpBe85n6zaeAtSaIbM5nyPitCovskfSoogYTr7kn61Q7nDgn4BP55NC8tz52sZ+SdcCn6wVj1m38RSg1m7S3sewAVgFXJH8vrW0gKRZwHeAr0fEzSXb8klF5PonHkkZj1nHqXaXqxND92vH2mLaPoYrgNMlPQ6cliwjaUDS2qTMOcBbgQ+WuSz1ekkPAw8D84HPpYzHrOO0+i5Xa52h0b1s3F08JtLG3a0fij1VjSEingfeXmb9IPDh5PF1wHUV9j81zeubdYM5fTPKJgEPrtf9tuwZpbSXN5L1raw1+C/PrMXa4fJEa41XKlz7U2l9s3isJLMW8+B61m6cGMzagAfX602zZoixyYOrB7NmVJjyr0nclGRm1iJvfM3hB30Jz0jWt5JrDGZmLdKuzYhODGZmLdSOzYhuSjIzsyJODGZmVsSJwczMijgxmJlZEScGMzMr4sRgZmZFnBjMzKyIE4OZmRVxYjAzsyKpEoOkIyV9T9Ljye9K8z1PFEzSs6Fg/TJJD0jaLummZLY3MzNrobQ1houBuyPieODuZLmcfRFxUvJzZsH6LwBXRsRvAC8CH0oZj5mZpZQ2MawE1ieP15Obt7kuyTzPpwL5eaCntL+ZmU2PtIlhYUQMJ493AwsrlJstaVDS/ZLyX/6vBl6KiPFkeSdwdMp4zMwspZqjq0q6CziqzKZPFy5EREiqNCHdsRGxS9JrgXskPQyMTiVQSauB1QBLliyZyq5mZjYFNRNDRJxWaZukPZIWRcSwpEXAsxWeY1fye4ek7wPLgW8D8yT1JbWGY4BdVeJYA6wBGBgYaPGMqGbtY2h0b9uN59+ruuVYpG1K2gCsSh6vAm4tLSDpCEmHJo/nA28GHo2IAO4F3lNtfzOrbGh0L5v2jLJvfBKAfeOTbNozytDo3hZH1nu66VikTQxXAKdLehw4LVlG0oCktUmZ1wGDkraQSwRXRMSjybaLgE9I2k6uz+GalPGY9ZStz73MREn9eSJy6625uulYpJrBLSKeB95eZv0g8OHk8U+AN1TYfwdwcpoYzHpZ/uy03vU2fbrpWPjOZ7MONqev/L+woCObMDpZpWNRaX0767yIzeyAE+cfxkwdvD6Awd2jbBp+qekx9apyx2Kmcus7jRODWQdb0j+X5Qv7KZMbAHjy5/tcc2iS/LHI1xDm9M1g+cL+jrwqKVUfg5m13pL+uQzurnxb0NbnXu7IL6dOtKR/bld81q4xmHWBau3Y+8YnXWuwKXFiMOsCtdqxO/V6emsNJwazLrCkfy7LDp9TcftEwEPP/ryJEVknc2Iw6xLLF81j4Kj+itvHJsO1BquLE4NZF1nSP7dqf0Mn3oVrzefEYNZlqvU3dOJduNZ8TgxmXWZJ/1wOqXBjQyfehWvN578Ssy702wv7u+YuXGs+3+Bm1oXyN1l1w9wA1nxODGZdqlvuwrXmc1OSmZkVSVVjkHQkcBOwFHgKOCciXiwp87vAlQWrfhM4NyL+j6SvAW/jV/M/fzAiNqeJycwsC90yTWcj0tYYLgbujojjgbuT5SIRcW9EnBQRJwGnAnuBfy4o8uf57U4KZtYOummazkakTQwrgfXJ4/XAWTXKvwe4IyJ649M1s47UTdN0NiJtYlgYEcPJ493AwhrlzwVuKFn3eUkPSbpS0qEp4zEzS62bpulsRM3EIOkuSY+U+VlZWC4igtzEUZWeZxG5uZ/vLFh9Cbk+h38PHAlcVGX/1ZIGJQ2OjIzUCtvMrGHdNE1nI2p2PkfEaZW2SdojaVFEDCdf/M9WeapzgO9ExCsFz52vbeyXdC3wySpxrAHWAAwMDFRMQGZmaZ04/zA27Rktak7qpRsE06a/DcCq5PEq4NYqZc+jpBkpSSZIErn+iUdSxmNmllo3TdPZiLQ3uF0BfFPSh4CnydUKkDQAfDQiPpwsLwUWA/+3ZP/rJS0ABGwGPpoyHjOzTPTyDYKpEkNEPA+8vcz6QeDDBctPAUeXKXdqmtc3M7Ps9UZPipmZ1c2JwczMingQPTMrq5eHhOh1TgxmdpD8kBD5yzXzQ0IAHZ0cnOzq46YkMztINw4J0evjH02FE4OZHaQbh4ToxmQ3XZwYzOwg3TgkRDcmu+nSuUfZzKbNifMP67o5o7sx2U0Xdz6b2UFqzRndiZ24vT7+0VQ4MZhZWZWGhOjUK5ZqJTv7FScGM5uSap247fAlW60208vjH02FE4OZTUk7d+J2am2m3bjXxcympJ07cX1JajZcYzCzKanVidvKjul2rs10EicGM5uSap24zWjKqZZ45vTNKJsE2qE200mcGMxsyip14k53x3StxONLUrORKo1Keq+krZImk1nbKpVbIWmbpO2SLi5Yv0zSA8n6myTNShOPmbVWPU05Q6N7ueOJPdyybZg7ntgzpbGKavUh9PqUnFlJW2N4BDgbuLpSAUkzgauA04GdwIOSNkTEo8AXgCsj4kZJ/wB8CPhKypjMrEVqNeWkbWqqJ/H4ktT0UtUYIuKxiNhWo9jJwPaI2BERY8CNwEpJAk4Fbk7KrQfOShOPmbVWraE00l411M5XRHWTZnyaRwPPFCzvTNa9GngpIsZL1pclabWkQUmDIyMj0xasmTWuVlNO2quGunEMp3ZUsylJ0l3AUWU2fToibs0+pPIiYg2wBmBgYCBqFDezFqnWlJP2qiEPa9EcNRNDRJyW8jV2AYsLlo9J1j0PzJPUl9Qa8uvNrEtlcdWQ+xCmXzOakh4Ejk+uQJoFnAtsiIgA7gXek5RbBTStBmJmzeerhjpDqquSJL0b+DKwAPgnSZsj4p2Sfh1YGxG/FxHjki4E7gRmAusiYmvyFBcBN0r6HLAJuCZNPGbW/nzG3/6UO3HvLAMDAzE4ONjqMMzMOoqkjRFR8Z6zPF/jZWZmRZwYzMysiBODmZkVcWIwM7MiTgxmZlakI69KkjQCPD2FXeYDz01TOM3k99Fe/D7ai99HbcdGxIJahToyMUyVpMF6LtFqd34f7cXvo734fWTHTUlmZlbEicHMzIr0SmJY0+oAMuL30V78PtqL30dGeqKPwczM6tcrNQYzM6tTVycGSSskbZO0XdLFrY6nEZLWSXpW0iOtjiUNSYsl3SvpUUlbJf1pq2NqhKTZkn4qaUvyPv57q2NKQ9JMSZsk3dbqWBol6SlJD0vaLKljR9eUNE/SzZL+RdJjkv5jy2Lp1qYkSTOBfwVOJzdt6IPAeRHxaEsDmyJJbwV+AXw9In6r1fE0StIiYFFE/EzSYcBG4KwOPB4CXhURv5B0CPAj4E8j4v4Wh9YQSZ8ABoDDI+JdrY6nEZKeAgYioqPvYZC0HvhhRKxN5q6ZGxEvtSKWbq4xnAxsj4gdETEG3AisbHFMUxYRPwBeaHUcaUXEcET8LHn8MvAYVeb4bleR84tk8ZDkpyPPriQdA/w+sLbVsfQ6Sf3AW0nmpImIsVYlBejuxHA08EzB8k468IuoG0laCiwHHmhtJI1Jml82A88C34uIjnwfwJeATwEHT8LcWQL4Z0kbJa1udTANWgaMANcmTXtrJb2qVcF0c2KwNiTp14BvA/81In7e6ngaERETEXESuXnKT5bUcU18kt4FPBsRG1sdSwbeEhFvAs4APp40v3aaPuBNwFciYjnw/4CW9Yt2c2LYBSwuWD4mWWctkrTJfxu4PiJuaXU8aSVV/XuBFa2OpQFvBs5M2udvBE6VdF1rQ2pMROxKfj8LfIdcM3Kn2QnsLKh93kwuUbRENyeGB4HjJS1LOnLOBTa0OKaelXTaXgM8FhH/o9XxNErSAknzksdzyF3c8C+tjWrqIuKSiDgmIpaS+9+4JyIuaHFYUybpVcnFDCRNL+8AOu4KvojYDTwj6YRk1duBll2Y0deqF55uETEu6ULgTmAmsC4itrY4rCmTdAPwO8B8STuBz0bENa2NqiFvBj4APJy0zwNcGhG3tzCmRiwC1idXvc0AvhkRHXupZxdYCHwnd95BH/C/I+K7rQ2pYX8CXJ+cyO4A/rBVgXTt5apmZtaYbm5KMjOzBjgxmJlZEScGMzMr4sRgZmZFnBjMzKyIE4OZmRVxYjAzsyJODGZmVuT/A/osdGtMrYLuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113ca6ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig =plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(X,Y,color='lightblue',marker='o')\n",
    "ax.scatter(X,  OUTo,\n",
    "               color='darkgreen',\n",
    "               marker='^')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still bad. Whether the model needs more complexity, or the gradient does not converge fast enough. let's try to train with a batch of only 10 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(X, Y):\n",
    "    for i in np.arange(0, X.shape[0], batchSize):\n",
    "        yield (X[i:i + batchSize], Y[i:i + batchSize])\n",
    "batchSize = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps:100 - loss:86.93324632780816\n",
      "steps:100 - loss:2.522219765001226\n",
      "steps:100 - loss:19.739745471125012\n",
      "steps:100 - loss:1.2363315269536002\n",
      "steps:100 - loss:6.991859701424015\n",
      "steps:200 - loss:55.94407737978787\n",
      "steps:200 - loss:148.96145738179516\n",
      "steps:200 - loss:216.0683001304852\n",
      "steps:200 - loss:2.746203453996073\n",
      "steps:200 - loss:42.17438446053611\n",
      "steps:300 - loss:25.83980458118609\n",
      "steps:300 - loss:111.54426314156382\n",
      "steps:300 - loss:210.42882328863527\n",
      "steps:300 - loss:0.17286228479983656\n",
      "steps:300 - loss:19.705765559889958\n",
      "steps:400 - loss:3.1598766211240874\n",
      "steps:400 - loss:32.555556596899564\n",
      "steps:400 - loss:88.89784772247701\n",
      "steps:400 - loss:40.60007152383304\n",
      "steps:400 - loss:66.38958189678168\n",
      "steps:500 - loss:3.286423122292767\n",
      "steps:500 - loss:32.062500577450415\n",
      "steps:500 - loss:86.6509483760393\n",
      "steps:500 - loss:40.903148507349286\n",
      "steps:500 - loss:66.9781011366177\n",
      "steps:600 - loss:3.3947654376967167\n",
      "steps:600 - loss:31.6646072684457\n",
      "steps:600 - loss:84.83976331760437\n",
      "steps:600 - loss:41.14181495526293\n",
      "steps:600 - loss:67.44238431190561\n",
      "steps:700 - loss:3.4880168279053647\n",
      "steps:700 - loss:31.337986259898095\n",
      "steps:700 - loss:83.36389748918897\n",
      "steps:700 - loss:41.336570977489345\n",
      "steps:700 - loss:67.83235510995699\n",
      "steps:800 - loss:3.568462665732788\n",
      "steps:800 - loss:31.066797396744295\n",
      "steps:800 - loss:82.14672547085749\n",
      "steps:800 - loss:41.49719290318355\n",
      "steps:800 - loss:68.16249388125625\n",
      "steps:900 - loss:3.637982291145067\n",
      "steps:900 - loss:30.839668743422372\n",
      "steps:900 - loss:81.13344981850571\n",
      "steps:900 - loss:41.63079063373363\n",
      "steps:900 - loss:68.44356848471563\n",
      "steps:1000 - loss:3.6981411096573953\n",
      "steps:1000 - loss:30.648153977763943\n",
      "steps:1000 - loss:80.28362222158363\n",
      "steps:1000 - loss:41.742684287514656\n",
      "steps:1000 - loss:68.68389566268029\n",
      "steps:1100 - loss:3.750255415667515\n",
      "steps:1100 - loss:30.485806215331618\n",
      "steps:1100 - loss:79.56660185952227\n",
      "steps:1100 - loss:41.836941839981016\n",
      "steps:1100 - loss:68.89006393131837\n",
      "steps:1200 - loss:3.795439971231666\n",
      "steps:1200 - loss:30.347594793848454\n",
      "steps:1200 - loss:78.95867797974256\n",
      "steps:1200 - loss:41.91672900299658\n",
      "steps:1200 - loss:69.06739242797651\n",
      "steps:1300 - loss:3.8346440120071192\n",
      "steps:1300 - loss:30.229523031252047\n",
      "steps:1300 - loss:78.44117652008886\n",
      "steps:1300 - loss:41.98454559945168\n",
      "steps:1300 - loss:69.22023766541633\n",
      "steps:1400 - loss:3.8686792046656233\n",
      "steps:1400 - loss:30.128368835892793\n",
      "steps:1400 - loss:77.9991730071538\n",
      "steps:1400 - loss:42.0423901769984\n",
      "steps:1400 - loss:69.35220757047144\n",
      "steps:1500 - loss:3.8982418251369095\n",
      "steps:1500 - loss:30.041503225400387\n",
      "steps:1500 - loss:77.62059245266231\n",
      "steps:1500 - loss:42.09187755753433\n",
      "steps:1500 - loss:69.46631622053803\n",
      "steps:1600 - loss:3.9239306657963913\n",
      "steps:1600 - loss:29.966759883760133\n",
      "steps:1600 - loss:77.29556404190666\n",
      "steps:1600 - loss:42.1343245657858\n",
      "steps:1600 - loss:69.5650990506012\n",
      "steps:1700 - loss:3.9462617034922514\n",
      "steps:1700 - loss:29.902339140170287\n",
      "steps:1700 - loss:77.0159482242797\n",
      "steps:1700 - loss:42.17081368639604\n",
      "steps:1700 - loss:69.65070075407174\n",
      "steps:1800 - loss:3.965680254222575\n",
      "steps:1800 - loss:29.846735781011585\n",
      "steps:1800 - loss:76.77498334080687\n",
      "steps:1800 - loss:42.20224107580476\n",
      "steps:1800 - loss:69.72494372885788\n",
      "steps:1900 - loss:3.9825711385915006\n",
      "steps:1900 - loss:29.79868376216551\n",
      "steps:1900 - loss:76.56701697398542\n",
      "steps:1900 - loss:42.22935327501493\n",
      "steps:1900 - loss:69.78938228577316\n",
      "steps:2000 - loss:3.9972672460893466\n",
      "steps:2000 - loss:29.757113169001517\n",
      "steps:2000 - loss:76.38729854895732\n",
      "steps:2000 - loss:42.25277562811853\n",
      "steps:2000 - loss:69.84534619210807\n",
      "steps:2100 - loss:4.010056792247216\n",
      "steps:2100 - loss:29.721116229709487\n",
      "steps:2100 - loss:76.23181702253761\n",
      "steps:2100 - loss:42.27303452422091\n",
      "steps:2100 - loss:69.89397606400371\n",
      "steps:2200 - loss:4.021189496275957\n",
      "steps:2200 - loss:29.689920142160997\n",
      "steps:2200 - loss:76.09717231034847\n",
      "steps:2200 - loss:42.29057498029618\n",
      "steps:2200 - loss:69.93625241936635\n",
      "steps:2300 - loss:4.030881858736986\n",
      "steps:2300 - loss:29.66286511269647\n",
      "steps:2300 - loss:75.98047233880189\n",
      "steps:2300 - loss:42.30577466860382\n",
      "steps:2300 - loss:69.97301972570322\n",
      "steps:2400 - loss:4.03932168323667\n",
      "steps:2400 - loss:29.639386440472837\n",
      "steps:2400 - loss:75.87924982564627\n",
      "steps:2400 - loss:42.31895520190682\n",
      "steps:2400 - loss:70.00500644487894\n",
      "steps:2500 - loss:4.046671959252033\n",
      "steps:2500 - loss:29.618999783466258\n",
      "steps:2500 - loss:75.79139443818359\n",
      "steps:2500 - loss:42.33039128291699\n",
      "steps:2500 - loss:70.03284184036079\n",
      "steps:2600 - loss:4.05307420245625\n",
      "steps:2600 - loss:29.6012889560853\n",
      "steps:2600 - loss:75.71509707276643\n",
      "steps:2600 - loss:42.340318175082146\n",
      "steps:2600 - loss:70.05707014087592\n",
      "steps:2700 - loss:4.0586513326160025\n",
      "steps:2700 - loss:29.58589576213869\n",
      "steps:2700 - loss:75.64880378622088\n",
      "steps:2700 - loss:42.348937842739375\n",
      "steps:2700 - loss:70.07816252758671\n",
      "steps:2800 - loss:4.0635101561135505\n",
      "steps:2800 - loss:29.572511479227245\n",
      "steps:2800 - loss:75.59117748379482\n",
      "steps:2800 - loss:42.35642402808829\n",
      "steps:2800 - loss:70.09652731660813\n",
      "steps:2900 - loss:4.067743509601423\n",
      "steps:2900 - loss:29.560869693894084\n",
      "steps:2900 - loss:75.54106589224065\n",
      "steps:2900 - loss:42.36292647230519\n",
      "steps:2900 - loss:70.11251863596331\n",
      "steps:3000 - loss:4.071432112651514\n",
      "steps:3000 - loss:29.550740249474988\n",
      "steps:3000 - loss:75.49747466396987\n",
      "steps:3000 - loss:42.3685744428501\n",
      "steps:3000 - loss:70.12644383984649\n",
      "steps:3100 - loss:4.07464617009936\n",
      "steps:3100 - loss:29.541924116270376\n",
      "steps:3100 - loss:75.45954469834092\n",
      "steps:3100 - loss:42.37347969463183\n",
      "steps:3100 - loss:70.13856985903426\n",
      "steps:3200 - loss:4.077446758811256\n",
      "steps:3200 - loss:29.534249030416007\n",
      "steps:3200 - loss:75.42653294988199\n",
      "steps:3200 - loss:42.377738966347195\n",
      "steps:3200 - loss:70.14912865137543\n",
      "steps:3300 - loss:4.079887028576336\n",
      "steps:3300 - loss:29.527565776508265\n",
      "steps:3300 - loss:75.39779613540122\n",
      "steps:3300 - loss:42.38143609299943\n",
      "steps:3300 - loss:70.15832188841439\n",
      "steps:3400 - loss:4.0820132425850275\n",
      "steps:3400 - loss:29.521745011614012\n",
      "steps:3400 - loss:75.37277686278117\n",
      "steps:3400 - loss:42.384643799771254\n",
      "steps:3400 - loss:70.16632499163109\n",
      "steps:3500 - loss:4.083865679355324\n",
      "steps:3500 - loss:29.516674546276473\n",
      "steps:3500 - loss:75.35099179169062\n",
      "steps:3500 - loss:42.387425230074946\n",
      "steps:3500 - loss:70.17329061349041\n",
      "steps:3600 - loss:4.085479414903877\n",
      "steps:3600 - loss:29.51225701254227\n",
      "steps:3600 - loss:75.33202150582355\n",
      "steps:3600 - loss:42.38983525081074\n",
      "steps:3600 - loss:70.1793516434059\n",
      "steps:3700 - loss:4.086885001345187\n",
      "steps:3700 - loss:29.508407860690717\n",
      "steps:3700 - loss:75.31550183182665\n",
      "steps:3700 - loss:42.39192157012839\n",
      "steps:3700 - loss:70.184623806361\n",
      "steps:3800 - loss:4.088109055859864\n",
      "steps:3800 - loss:29.505053635867867\n",
      "steps:3800 - loss:75.30111638495413\n",
      "steps:3800 - loss:42.393725696790845\n",
      "steps:3800 - loss:70.18920791162417\n",
      "steps:3900 - loss:4.089174772061826\n",
      "steps:3900 - loss:29.502130493592343\n",
      "steps:3900 - loss:75.2885901577779\n",
      "steps:3900 - loss:42.395283765241125\n",
      "steps:3900 - loss:70.19319180038795\n",
      "steps:4000 - loss:4.090102364140944\n",
      "steps:4000 - loss:29.4995829195501\n",
      "steps:4000 - loss:75.27768399810766\n",
      "steps:4000 - loss:42.39662724646044\n",
      "steps:4000 - loss:70.19665203400983\n",
      "steps:4100 - loss:4.090909452749063\n",
      "steps:4100 - loss:29.49736262440147\n",
      "steps:4100 - loss:75.26818984661078\n",
      "steps:4100 - loss:42.39778356140587\n",
      "steps:4100 - loss:70.19965535843946\n",
      "steps:4200 - loss:4.0916114003751245\n",
      "steps:4200 - loss:29.495427588777986\n",
      "steps:4200 - loss:75.25992662486516\n",
      "steps:4200 - loss:42.39877661114312\n",
      "steps:4200 - loss:70.20225997533643\n",
      "steps:4300 - loss:4.092221602910235\n",
      "steps:4300 - loss:29.49374123734303\n",
      "steps:4300 - loss:75.25273668126904\n",
      "steps:4300 - loss:42.399627235569575\n",
      "steps:4300 - loss:70.20451664604776\n",
      "steps:4400 - loss:4.092751743199251\n",
      "steps:4400 - loss:29.492271723917472\n",
      "steps:4400 - loss:75.24648271622961\n",
      "steps:4400 - loss:42.40035361079644\n",
      "steps:4400 - loss:70.2064696509178\n",
      "steps:4500 - loss:4.093212011596031\n",
      "steps:4500 - loss:29.490991312289015\n",
      "steps:4500 - loss:75.2410451197074\n",
      "steps:4500 - loss:42.40097159374217\n",
      "steps:4500 - loss:70.20815762330434\n",
      "steps:4600 - loss:4.093611297866525\n",
      "steps:4600 - loss:29.489875839550834\n",
      "steps:4600 - loss:75.2363196640468\n",
      "steps:4600 - loss:42.40149502122018\n",
      "steps:4600 - loss:70.20961427498996\n",
      "steps:4700 - loss:4.0939573582060955\n",
      "steps:4700 - loss:29.4889042506845\n",
      "steps:4700 - loss:75.2322155032517\n",
      "steps:4700 - loss:42.401935969730836\n",
      "steps:4700 - loss:70.21086902736361\n",
      "steps:4800 - loss:4.094256960632347\n",
      "steps:4800 - loss:29.488058194706884\n",
      "steps:4800 - loss:75.22865343689972\n",
      "steps:4800 - loss:42.40230498128653\n",
      "steps:4800 - loss:70.21194756083479\n",
      "steps:4900 - loss:4.094516011582839\n",
      "steps:4900 - loss:29.48732167406243\n",
      "steps:4900 - loss:75.22556440282563\n",
      "steps:4900 - loss:42.402611259834515\n",
      "steps:4900 - loss:70.21287229322736\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,steps):\n",
    "    for (Xb, Yb) in next_batch(X, Y):\n",
    "        OUTh = tanh(np.dot(Xb, layer1_W))  # out of layer1 which is a 50x4 dimension\n",
    "\n",
    "        OUTo = tanh(np.dot(OUTh, layer2_W))  # prediction of the system (50x1)\n",
    "\n",
    "        # backprog we will use the Sqare error function = Sigma(y_hat-y)^2\n",
    "\n",
    "        # for layer2:\n",
    "        dEtotal_dOUTo = -(Yb-OUTo)  # dimension (50,1)\n",
    "        dOUTo_dNETo = dtanh(OUTo)#OUTo*(1-OUTo)  # dimension (50,1)\n",
    "        #OUTh   dimension (50,4)\n",
    "\n",
    "        dEtotal_dOUTh = np.dot(-(Yb-OUTo)*dtanh(OUTo),layer2_W.T)  # dimension (50,1)\n",
    "        dOUTh_dNETh = dtanh(OUTh)  # dimension (50,4)\n",
    "        dNETh_dW = X # dimension (50,1)\n",
    "\n",
    "        #layer2_W = layer2_W - alpha * (np.dot(dEtotal_dOUTo.T,dOUTo_dNETo*OUTh)).T ##OLD\n",
    "\n",
    "        layer2_W = layer2_W - alpha * (np.dot((dEtotal_dOUTo*dOUTo_dNETo).T,OUTh)).T\n",
    "\n",
    "        # for layer1:\n",
    "        #layer1_W = layer1_W - alpha * (np.dot((dEtotal_dOUTh * dOUTh_dNETh).T, dNETh_dW)).T  ##OLD\n",
    "        layer1_W = layer1_W - alpha * (Xb.T.dot(dEtotal_dOUTh*dOUTh_dNETh))\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            OUTh = tanh(np.dot(X, layer1_W))  # out of layer1 which is a 50x4 dimension\n",
    "            OUTo = tanh(np.dot(OUTh, layer2_W))\n",
    "\n",
    "            print(\"steps:{0} - loss:{1}\".format(i, 0.5*(np.sum(Y-OUTo))*(np.sum(Y-OUTo))))\n",
    "        OUTh = tanh(np.dot(X, layer1_W))  # out of layer1 which is a 50x4 dimension\n",
    "        OUTo = tanh(np.dot(OUTh, layer2_W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114113e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig =plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(X,Y,color='lightblue',marker='o')\n",
    "ax.scatter(X,  OUTo,\n",
    "               color='darkgreen',\n",
    "               marker='^')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouah seems much better ! Last point we forgot is to add a bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = randSinVector(n)\n",
    "X = data[:,0] # will give dimension of (n,), we need (n,1)\n",
    "# see https://stackoverflow.com/questions/36412762/how-to-understand-empty-dimension-in-python-numpy-array\n",
    "X = X[np.newaxis].T # X is a (n,1)  \n",
    "Y = data[:,1]\n",
    "Y = Y[np.newaxis].T\n",
    "\n",
    "\n",
    "# add a bias unit to the input layer\n",
    "X = np.concatenate((np.atleast_2d(np.ones(X.shape[0])).T, X), axis=1)\n",
    "\n",
    "#layer1_W is the weight vector from input to hidden layer\n",
    "layer1_W = np.random.random((2,8))  # dim = input , number node\n",
    "#layer2_W is the weight vector from layer1 to output layer\n",
    "layer2_W = np.random.random((8,1)) # dim = number node , output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps:100 - loss:4.9066584767592625\n",
      "steps:100 - loss:3.608591795472601\n",
      "steps:100 - loss:14.333121970502171\n",
      "steps:100 - loss:0.39562993890635567\n",
      "steps:100 - loss:0.48799040178311687\n",
      "steps:200 - loss:4.271209758466033\n",
      "steps:200 - loss:4.86672801635302\n",
      "steps:200 - loss:16.033324405871813\n",
      "steps:200 - loss:2.6520300849762197\n",
      "steps:200 - loss:0.30410922683334574\n",
      "steps:300 - loss:4.655282636633231\n",
      "steps:300 - loss:4.828051107922977\n",
      "steps:300 - loss:17.365112183349183\n",
      "steps:300 - loss:3.2715823930235954\n",
      "steps:300 - loss:0.9439083549612683\n",
      "steps:400 - loss:3.446330272842241\n",
      "steps:400 - loss:5.890826340970384\n",
      "steps:400 - loss:20.487712127936298\n",
      "steps:400 - loss:5.113260444294991\n",
      "steps:400 - loss:0.2829678663296968\n",
      "steps:500 - loss:2.9875331865464947\n",
      "steps:500 - loss:5.493786235825773\n",
      "steps:500 - loss:22.257551120302043\n",
      "steps:500 - loss:4.9985860750737245\n",
      "steps:500 - loss:0.5593822482068613\n",
      "steps:600 - loss:14.420027303650244\n",
      "steps:600 - loss:0.21276637091023018\n",
      "steps:600 - loss:4.121092506079688\n",
      "steps:600 - loss:0.34360745448887153\n",
      "steps:600 - loss:28.643751331985275\n",
      "steps:700 - loss:11.746863438807711\n",
      "steps:700 - loss:0.2918297425678637\n",
      "steps:700 - loss:5.020600412304577\n",
      "steps:700 - loss:0.30470532400361805\n",
      "steps:700 - loss:25.354364714126053\n",
      "steps:800 - loss:10.034070089530786\n",
      "steps:800 - loss:0.4029703861194649\n",
      "steps:800 - loss:5.364104857604532\n",
      "steps:800 - loss:0.2687317567442057\n",
      "steps:800 - loss:23.43960233184274\n",
      "steps:900 - loss:7.838661886342386\n",
      "steps:900 - loss:0.5039544891452403\n",
      "steps:900 - loss:4.9707030330720885\n",
      "steps:900 - loss:0.25050265839334007\n",
      "steps:900 - loss:20.613936317146635\n",
      "steps:1000 - loss:5.081402432327725\n",
      "steps:1000 - loss:0.6416892823975258\n",
      "steps:1000 - loss:3.318315395412448\n",
      "steps:1000 - loss:0.3219629494891525\n",
      "steps:1000 - loss:16.349430426804645\n",
      "steps:1100 - loss:4.697745981346014\n",
      "steps:1100 - loss:1.6051563237628375\n",
      "steps:1100 - loss:1.829371849336572\n",
      "steps:1100 - loss:1.4805032459158822\n",
      "steps:1100 - loss:13.766897135217071\n",
      "steps:1200 - loss:1.1180203367338142\n",
      "steps:1200 - loss:0.5192482421552542\n",
      "steps:1200 - loss:0.228213915770684\n",
      "steps:1200 - loss:2.79538368201044\n",
      "steps:1200 - loss:3.632910882267753\n",
      "steps:1300 - loss:0.46840774194839885\n",
      "steps:1300 - loss:1.009033667615166\n",
      "steps:1300 - loss:0.007462134355163114\n",
      "steps:1300 - loss:1.6961417375503405\n",
      "steps:1300 - loss:5.422374956358784\n",
      "steps:1400 - loss:0.9536820044588634\n",
      "steps:1400 - loss:1.0215821825321396\n",
      "steps:1400 - loss:0.08842736461101351\n",
      "steps:1400 - loss:1.8586485022562151\n",
      "steps:1400 - loss:4.5236743820870196\n",
      "steps:1500 - loss:1.2751055411980952\n",
      "steps:1500 - loss:1.0026639293751443\n",
      "steps:1500 - loss:0.14903655022202533\n",
      "steps:1500 - loss:1.9777519969113324\n",
      "steps:1500 - loss:3.4145754340676766\n",
      "steps:1600 - loss:1.324203254212509\n",
      "steps:1600 - loss:0.864848872368443\n",
      "steps:1600 - loss:0.13774474992901942\n",
      "steps:1600 - loss:1.9433071876516408\n",
      "steps:1600 - loss:2.713188101679247\n",
      "steps:1700 - loss:1.2931189194987136\n",
      "steps:1700 - loss:0.7291947825275239\n",
      "steps:1700 - loss:0.11363935993498368\n",
      "steps:1700 - loss:1.8088681087122211\n",
      "steps:1700 - loss:2.270674558854637\n",
      "steps:1800 - loss:1.234008155051199\n",
      "steps:1800 - loss:0.6139289083370526\n",
      "steps:1800 - loss:0.09098803038598217\n",
      "steps:1800 - loss:1.636866633456999\n",
      "steps:1800 - loss:1.9224381675988222\n",
      "steps:1900 - loss:1.1485338922931911\n",
      "steps:1900 - loss:0.509297689360065\n",
      "steps:1900 - loss:0.07005193759095388\n",
      "steps:1900 - loss:1.4500632146816486\n",
      "steps:1900 - loss:1.6091110332247902\n",
      "steps:2000 - loss:1.038929581848441\n",
      "steps:2000 - loss:0.41380626393615455\n",
      "steps:2000 - loss:0.05170150651789386\n",
      "steps:2000 - loss:1.2594347544140485\n",
      "steps:2000 - loss:1.321155442503805\n",
      "steps:2100 - loss:0.9123169860142455\n",
      "steps:2100 - loss:0.32978331906698444\n",
      "steps:2100 - loss:0.03680174243427105\n",
      "steps:2100 - loss:1.0741194871030477\n",
      "steps:2100 - loss:1.0629436781943034\n",
      "steps:2200 - loss:0.777060128231264\n",
      "steps:2200 - loss:0.2587139039264183\n",
      "steps:2200 - loss:0.02547584767646514\n",
      "steps:2200 - loss:0.901117393744026\n",
      "steps:2200 - loss:0.8382232930062725\n",
      "steps:2300 - loss:0.6389926701299679\n",
      "steps:2300 - loss:0.19974043078374415\n",
      "steps:2300 - loss:0.017199376843895935\n",
      "steps:2300 - loss:0.7432916402844523\n",
      "steps:2300 - loss:0.6454017378072446\n",
      "steps:2400 - loss:0.4997319748916405\n",
      "steps:2400 - loss:0.1502662792145115\n",
      "steps:2400 - loss:0.011222683681108764\n",
      "steps:2400 - loss:0.5988582303397589\n",
      "steps:2400 - loss:0.4782748979297674\n",
      "steps:2500 - loss:0.7317799139809494\n",
      "steps:2500 - loss:0.24906475739433512\n",
      "steps:2500 - loss:0.012345844147532597\n",
      "steps:2500 - loss:0.8579192852765077\n",
      "steps:2500 - loss:0.8035180137937127\n",
      "steps:2600 - loss:0.09296847731340614\n",
      "steps:2600 - loss:0.07643960004583629\n",
      "steps:2600 - loss:0.016366066907348115\n",
      "steps:2600 - loss:0.10212783573157692\n",
      "steps:2600 - loss:0.08848101109618628\n",
      "steps:2700 - loss:0.08251816220731811\n",
      "steps:2700 - loss:0.06808851184221433\n",
      "steps:2700 - loss:0.014829766539817697\n",
      "steps:2700 - loss:0.08525958988350056\n",
      "steps:2700 - loss:0.07080175117504912\n",
      "steps:2800 - loss:0.07235817250246801\n",
      "steps:2800 - loss:0.06134775795006592\n",
      "steps:2800 - loss:0.01353785798024737\n",
      "steps:2800 - loss:0.07133868387716984\n",
      "steps:2800 - loss:0.0565305331152624\n",
      "steps:2900 - loss:0.06261181272538865\n",
      "steps:2900 - loss:0.055794367179290115\n",
      "steps:2900 - loss:0.012436014177481506\n",
      "steps:2900 - loss:0.05967831722169232\n",
      "steps:2900 - loss:0.04480604246421648\n",
      "steps:3000 - loss:0.053477000051329326\n",
      "steps:3000 - loss:0.051178701686009964\n",
      "steps:3000 - loss:0.011487967564517415\n",
      "steps:3000 - loss:0.049908114451825636\n",
      "steps:3000 - loss:0.03514657150377373\n",
      "steps:3100 - loss:0.045074835485894485\n",
      "steps:3100 - loss:0.04732104772011455\n",
      "steps:3100 - loss:0.010667409412421325\n",
      "steps:3100 - loss:0.041723645420468004\n",
      "steps:3100 - loss:0.027200386477873192\n",
      "steps:3200 - loss:0.037471612442964757\n",
      "steps:3200 - loss:0.04408545565855276\n",
      "steps:3200 - loss:0.009953785516913024\n",
      "steps:3200 - loss:0.034871027089676876\n",
      "steps:3200 - loss:0.020695700721350557\n",
      "steps:3300 - loss:0.030693193025821786\n",
      "steps:3300 - loss:0.04136536559800098\n",
      "steps:3300 - loss:0.009330260169992518\n",
      "steps:3300 - loss:0.02913678968394492\n",
      "steps:3300 - loss:0.015413430471141973\n",
      "steps:3400 - loss:0.024735286842763788\n",
      "steps:3400 - loss:0.03907513240106108\n",
      "steps:3400 - loss:0.00878269362749095\n",
      "steps:3400 - loss:0.024340742322875957\n",
      "steps:3400 - loss:0.011171578650535486\n",
      "steps:3500 - loss:0.019571441880288518\n",
      "steps:3500 - loss:0.03714472652921689\n",
      "steps:3500 - loss:0.008299093857760502\n",
      "steps:3500 - loss:0.020330661067327272\n",
      "steps:3500 - loss:0.0078159216892675\n",
      "steps:3600 - loss:0.01515958039328244\n",
      "steps:3600 - loss:0.035516239297808276\n",
      "steps:3600 - loss:0.007869284734763933\n",
      "steps:3600 - loss:0.016978112166747463\n",
      "steps:3600 - loss:0.005214224562200326\n",
      "steps:3700 - loss:0.011447416457454769\n",
      "steps:3700 - loss:0.034141455774746636\n",
      "steps:3700 - loss:0.007484668582128702\n",
      "steps:3700 - loss:0.014175005117532377\n",
      "steps:3700 - loss:0.003252451389647813\n",
      "steps:3800 - loss:0.008376876768421278\n",
      "steps:3800 - loss:0.0329800813441302\n",
      "steps:3800 - loss:0.0071380283026586284\n",
      "steps:3800 - loss:0.011830644711882676\n",
      "steps:3800 - loss:0.001832100358458186\n",
      "steps:3900 - loss:0.005887585746174856\n",
      "steps:3900 - loss:0.03199838354500636\n",
      "steps:3900 - loss:0.006823347523342721\n",
      "steps:3900 - loss:0.009869158634183573\n",
      "steps:3900 - loss:0.0008681691088037676\n",
      "steps:4000 - loss:0.003919486208113598\n",
      "steps:4000 - loss:0.031168110495620825\n",
      "steps:4000 - loss:0.00653564270103175\n",
      "steps:4000 - loss:0.008227238418590617\n",
      "steps:4000 - loss:0.00028747974267196477\n",
      "steps:4100 - loss:0.0024146965294352873\n",
      "steps:4100 - loss:0.030465604670081543\n",
      "steps:4100 - loss:0.006270807452155273\n",
      "steps:4100 - loss:0.006852161884040626\n",
      "steps:4100 - loss:2.7223804613950876e-05\n",
      "steps:4200 - loss:0.0013187282819086365\n",
      "steps:4200 - loss:0.029871063614823525\n",
      "steps:4200 - loss:0.006025471100611392\n",
      "steps:4200 - loss:0.00570007662850124\n",
      "steps:4200 - loss:3.366154165460244e-05\n",
      "steps:4300 - loss:0.0005811961279752634\n",
      "steps:4300 - loss:0.029367917504767797\n",
      "steps:4300 - loss:0.005796873125601479\n",
      "steps:4300 - loss:0.004734525890864946\n",
      "steps:4300 - loss:0.00026094814762548453\n",
      "steps:4400 - loss:0.0001561452586535894\n",
      "steps:4400 - loss:0.028942303391444626\n",
      "steps:4400 - loss:0.0055827541949651106\n",
      "steps:4400 - loss:0.003925196087320123\n",
      "steps:4400 - loss:0.0006700770052638125\n",
      "steps:4500 - loss:2.1057788461696805e-06\n",
      "steps:4500 - loss:0.02858262133839605\n",
      "steps:4500 - loss:0.005381263453353101\n",
      "steps:4500 - loss:0.0032468629628023583\n",
      "steps:4500 - loss:0.0012279357478440737\n",
      "steps:4600 - loss:8.196336336543686e-05\n",
      "steps:4600 - loss:0.028279160589227765\n",
      "steps:4600 - loss:0.005190880963764712\n",
      "steps:4600 - loss:0.0026785120306411906\n",
      "steps:4600 - loss:0.001906471295443769\n",
      "steps:4700 - loss:0.00036271517584112985\n",
      "steps:4700 - loss:0.028023785708002392\n",
      "steps:4700 - loss:0.0050103537318247505\n",
      "steps:4700 - loss:0.0022026091281293746\n",
      "steps:4700 - loss:0.002681958355402662\n",
      "steps:4800 - loss:0.0008151618204798377\n",
      "steps:4800 - loss:0.02780967390914493\n",
      "steps:4800 - loss:0.004838643535972111\n",
      "steps:4800 - loss:0.0018044982620723186\n",
      "steps:4800 - loss:0.0035343639901034146\n",
      "steps:4900 - loss:0.0014135710274255434\n",
      "steps:4900 - loss:0.02763109585262246\n",
      "steps:4900 - loss:0.004674884772807379\n",
      "steps:4900 - loss:0.0014719060443906644\n",
      "steps:4900 - loss:0.004446799493494319\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,steps):\n",
    "    for (Xb, Yb) in next_batch(X, Y):\n",
    "        OUTh = tanh(np.dot(Xb, layer1_W))  # out of layer1 which is a 50x4 dimension\n",
    "\n",
    "        OUTo = tanh(np.dot(OUTh, layer2_W))  # prediction of the system (50x1)\n",
    "\n",
    "        # backprog we will use the Sqare error function = Sigma(y_hat-y)^2\n",
    "\n",
    "        # for layer2:\n",
    "        dEtotal_dOUTo = -(Yb-OUTo)  # dimension (50,1)\n",
    "        dOUTo_dNETo = dtanh(OUTo)#OUTo*(1-OUTo)  # dimension (50,1)\n",
    "        #OUTh   dimension (50,4)\n",
    "\n",
    "        dEtotal_dOUTh = np.dot(-(Yb-OUTo)*dtanh(OUTo),layer2_W.T)  # dimension (50,1)\n",
    "        dOUTh_dNETh = dtanh(OUTh)  # dimension (50,4)\n",
    "        dNETh_dW = X # dimension (50,1)\n",
    "\n",
    "        #layer2_W = layer2_W - alpha * (np.dot(dEtotal_dOUTo.T,dOUTo_dNETo*OUTh)).T ##OLD\n",
    "\n",
    "        layer2_W = layer2_W - alpha * (np.dot((dEtotal_dOUTo*dOUTo_dNETo).T,OUTh)).T\n",
    "\n",
    "        # for layer1:\n",
    "        #layer1_W = layer1_W - alpha * (np.dot((dEtotal_dOUTh * dOUTh_dNETh).T, dNETh_dW)).T  ##OLD\n",
    "        layer1_W = layer1_W - alpha * (Xb.T.dot(dEtotal_dOUTh*dOUTh_dNETh))\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            OUTh = tanh(np.dot(X, layer1_W))  # out of layer1 which is a 50x4 dimension\n",
    "            OUTo = tanh(np.dot(OUTh, layer2_W))\n",
    "\n",
    "            print(\"steps:{0} - loss:{1}\".format(i, 0.5*(np.sum(Y-OUTo))*(np.sum(Y-OUTo))))\n",
    "        OUTh = tanh(np.dot(X, layer1_W))  # out of layer1 which is a 50x4 dimension\n",
    "        OUTo = tanh(np.dot(OUTh, layer2_W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1141a52b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig =plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(X[:,1],Y,color='lightblue',marker='o')\n",
    "ax.scatter(X[:,1],  OUTo,\n",
    "               color='darkgreen',\n",
    "               marker='^')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
